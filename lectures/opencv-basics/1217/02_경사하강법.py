# 경사 하강법
# 가장 낮은 곳을 찾아가는 방법
# 

# 안개 낀 산에서 내려오기
# 안개 낀 산 정상 
# -> 마을은 가장 낮은 곳에 있는데 앞이 보이지 않아서 어디로 가야할지 모릅니다.

# 발밑의 지형만 느끼면서 "지금 서있는곳에서 가장 가파르게 내려가는 방향"
# 을 찾아 한걸음씩 내딛는거

# 머신러닝에서 모델을 학습시킨다는 건 "최적의 파라미터 값을 찾는다"라는 의미
# 예를 들면 집 가격을 예측하는 모델이 있다면, 예측값과 실제 가격의 차이(오차)를
# 최소화하는 파라미터를 찾아야함

# 이 오차를 수치화한 것이 손실함수 이고 
# 경사 하강법은 이 손실 함수의 값을 최소화하는 파라미터를 찾아가는 알고리즘

# 작동 원리 
# 경사 하강법
# θ(새로운) = θ(현재) - α × ∇L(θ)
# 
# θ(세타): 우리가 찾고자 하는 파라미터
# α(알파): 학습률, 한번에 얼마나 크게 이동할지 결정
# ∇L(θ): 현재 위치에서 손실 함수의 기울기(gradient)

# 기울기에 마이너를 붙이는 이유는? 기울기는 함수가 증가하는 방향으로 가리키기 때문에
# 반대로 가야 감소하는 방향이 되기 때문

# 학습률의 중요성
# 학습률 α는 매우 중요한 하이퍼파라미터

# 학습률이 너무 크면 최저점을 지나쳐서 발산할수 있음
# 산을 내려오는데 너무 큰 보폭으로 뛰어다니면 반대편 산으로 넘어가 버림

# 학습률이 너무 작으면 수렴하는데 너무 오래 걸림
# 아주 작은 걸음으로 조금씩 이동하니 시간이 한참 걸림

# 한계점
# 지역 최솟값 문제
# 경사 하강법은 "현재 위치에서 가장 가파른 방향" 만 보고 이동하기 떄문에,
# 전체에서 가장 낮은곳(전역 최솟값, Global Minmum)이 아니라 근처의 움푹 파인곳
# (지역 최솟값, Local Minmum)에 갇힐 수 있다.

# SGD(확률적 경사 하강법): 전체 데이터 대신 일부 샘플만 사용해서 더 빠르게 업데이트하고,
# 약간의 무작위성이 지역 최솟값 탈출에 도움을 준다.

# Momentum: 이전 이동 방향의 관성을 반영해서 지역 최솟값을 넘어갈수 있게 해줌

# Adam: 학습률을 파라미터별로 적응적으로 조절하는 방법으로,
# 현재 가장 널리 쓰이는 옵티마이저 중 하나

import numpy as np
import matplotlib.pyplot as plt

# 데이터 생성
np.random.seed(42)
x = np.random.randn(100)
y = 2 * x + 3 + np.random.randn(100) * 0.5 # y = 2x + 3 + 노이즈

# 파라미터 초기화
w = 0.0
b = 0.0
lr = 0.1
epochs = 100

# 기록용
loss_history = []
w_history = []
b_history = []

# 경사 하강법
for epoch in range(epochs):
    # 예측
    y_pred = w * x + b

    # 손실 계산 (MSE)
    loss = np.mean((y - y_pred) ** 2)
    loss_history.append(loss) 

    # 그래디언트 계산
    dw = np.mean(2 * (y_pred - y) * x)
    db = np.mean(2 * (y_pred - y))

    # 파라미터 업데이트 
    w = w - lr * dw
    b = b - lr * db

    # 기록
    w_history.append(w)
    b_history.append(b)

    if epoch % 20 == 0:
        print(f'Epoch {epoch:3d}: Loss={loss:.4f}, w={w:.4f}, b={b:.4f}')

print(f'최종:  w={w:.4f} (목표 : 2), b={b:.4f} (목표 : 3)')