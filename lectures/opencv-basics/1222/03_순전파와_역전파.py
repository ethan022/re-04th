# 신경망이 학습하는 방법

# 전체 흐름 (쉽게!)
# 1. 순전파: 예측하기
# 2. 오차 계산: 얼마나 틀렸나?
# 3. 역전파: 어디를 고쳐야 할까?
# 4. 업데이트: 실제로 고치기


# 순전파 (Forward Propagation)
# = 입력 → 출력 방향으로 계산하기

# 비유: 공장 생산 라인

# [원료] → [1단계 가공] → [2단계 조립] → [3단계 포장] → [완제품]

# 신경망도 똑같음:
# [입력] → [은닉층1] → [은닉층2] → [은닉층3] → [출력]

# 예시: 숫자 예측하기
# 입력: 2
# 1층: 2 × 3 = 6
# 2층: 6 × 2 = 12
# 출력: 12

print("=== 순전파 예시 ===")
print("입력: 2")
print("1층: 2 × 3 = 6")
print("2층: 6 × 2 = 12")
print("예측 결과: 12")
print()


# 오차 계산
# = 예측과 정답의 차이

# 비유: 다트 던지기
# - 목표: 정중앙 (정답)
# - 실제: 조금 왼쪽에 맞음 (예측)
# - 오차: 얼마나 벗어났는지

print("=== 오차 계산 예시 ===")
prediction = 12  # 예측
target = 10      # 정답
error = prediction - target
print(f"예측: {prediction}")
print(f"정답: {target}")
print(f"오차: {error} (2만큼 틀렸네요!)")
print()


# 역전파 (Backpropagation)
# = 뒤에서부터 거꾸로 가면서 "누구 책임인지" 찾기

# 비유: 잘못된 제품이 나왔을 때

# [완제품] ← "어? 불량품이네!" (오차 발견)
# [3단계] ← "포장 때문일까?" (확인)
# [2단계] ← "조립이 잘못됐나?" (확인)
# [1단계] ← "아! 가공이 문제였구나!" (원인 발견)

# 신경망도 똑같음:
# [출력] ← "12가 나왔는데 10이어야 하는데?" (오차 2)
# [은닉층2] ← "여기서 너무 크게 만들었나?"
# [은닉층1] ← "여기부터 조정이 필요하겠네!"


# 왜 거꾸로 갈까?

# 예시: 접시 깨진 사건 수사
# 1. 접시가 깨졌음 (결과, 출력층)
# 2. 누가 마지막에 만졌지? (은닉층2)
# 3. 그 전에는? (은닉층1)
# 4. 처음 원인은? (입력층)

# → 결과부터 보고 거슬러 올라가는 게 쉬움!


# 학습 과정 (전체 흐름)

print("=== 신경망 학습 과정 ===")
print()
print("1단계: 순전파 (예측하기)")
print("   입력 2 → [신경망] → 출력 12")
print()
print("2단계: 오차 계산")
print("   예측 12 vs 정답 10 → 오차 2")
print()
print("3단계: 역전파 (책임 찾기)")
print("   '2층 가중치를 좀 줄여야겠다'")
print("   '1층 가중치도 조금 조정하자'")
print()
print("4단계: 업데이트 (실제로 고치기)")
print("   2층 가중치: 2.0 → 1.9")
print("   1층 가중치: 3.0 → 2.9")
print()
print("5단계: 다시 순전파 (개선됐나 확인)")
print("   입력 2 → [조정된 신경망] → 출력 10.8")
print("   오차가 2에서 0.8로 줄었어요!")
print()


# 경사하강법 (Gradient Descent)
# = 산을 내려가듯이 오차를 줄이는 방법

# 비유: 안개 낀 산에서 내려오기

# 상황: 눈을 가리고 산 정상에 있음
# 목표: 제일 낮은 곳(오차 최소)으로 가기

# 방법:
# 1. 발로 주변을 더듬음 (오차 확인)
# 2. 아래로 내려가는 방향 찾기 (경사 계산)
# 3. 그 방향으로 한 걸음 (가중치 업데이트)
# 4. 반복!

print("=== 경사하강법 비유 ===")
print()
print("현재 위치(가중치): 5.0")
print("오차: 크다!")
print()
print("한 걸음 내려감...")
print("새 위치(가중치): 4.5")
print("오차: 줄었다!")
print()
print("또 한 걸음 내려감...")
print("새 위치(가중치): 4.1")
print("오차: 더 줄었다!")
print()
print("계속 반복하면... 제일 낮은 곳에 도착!")
print()


# 학습률 (Learning Rate)
# = 한 걸음의 크기

# 비유: 산에서 내려올 때 걸음 크기

# 학습률이 크면 (큰 걸음):
# ✓ 빠르게 내려감
# ✗ 너무 커서 왔다갔다 할 수 있음
# ✗ 최적점을 넘어갈 수 있음

# 학습률이 작으면 (작은 걸음):
# ✓ 정확하게 내려감
# ✗ 너무 느림
# ✗ 시간이 오래 걸림

print("=== 학습률 효과 ===")
print()
print("학습률 = 0.1 (큰 걸음)")
print("5.0 → 4.5 → 4.0 → 3.5 (빠름!)")
print()
print("학습률 = 0.01 (작은 걸음)")
print("5.0 → 4.99 → 4.98 → 4.97 (느림...)")
print()
print("적절한 학습률 = 0.05 정도가 좋음!")
print()


# 간단한 학습 시뮬레이션

print("=== 신경망 학습 시뮬레이션 ===")
print()

# 초기 설정
weight = 5.0  # 가중치
target = 10   # 목표값
learning_rate = 0.1

print(f"초기 가중치: {weight}")
print(f"목표: {target}")
print(f"학습률: {learning_rate}")
print()

# 5번 학습
for epoch in range(1, 6):
    # 순전파: 예측
    prediction = weight * 2  # 입력이 2라고 가정

    # 오차 계산
    error = prediction - target

    # 역전파 & 업데이트
    # (실제로는 복잡하지만, 여기서는 간단하게)
    weight = weight - learning_rate * error * 0.1

    print(f"에폭 {epoch}:")
    print(f"  예측: {prediction:.2f}, 오차: {error:.2f}")
    print(f"  가중치 업데이트: {weight:.2f}")
    print()

print("학습 완료! 가중치가 조정되어 오차가 줄어들었습니다!")


# 핵심 정리

print("\n" + "="*60)
print("핵심 정리")
print("="*60)
print()
print("1. 순전파 = 입력부터 출력까지 계산 (예측하기)")
print()
print("2. 오차 계산 = 예측과 정답의 차이")
print()
print("3. 역전파 = 거꾸로 가면서 누구 책임인지 찾기")
print()
print("4. 경사하강법 = 산 내려가듯이 오차 줄이기")
print()
print("5. 학습률 = 한 걸음의 크기 (보통 0.001 ~ 0.1)")
print()
print("="*60)
print()
print("비유로 기억하기:")
print("- 순전파 = 공장 생산 라인")
print("- 역전파 = 불량품 원인 찾기")
print("- 경사하강법 = 안개 낀 산 내려오기")
print("="*60)
