# 신경망의 구조 이해하기

# 뇌의 뉴런 vs 인공 뉴런

# 우리 뇌는 어떻게 작동할까?
# 1. 눈으로 사과를 봄 → 2. 뇌 신경세포(뉴런)들이 신호를 주고받음
# → 3. "아! 사과다!" 하고 인식

# 뉴런의 역할:
# - 정보를 받음 (눈으로 본 모양, 색깔)
# - 정보를 처리함 ("이건 빨갛고 둥글네?")
# - 다음 뉴런에게 전달 ("사과일 것 같아!")


# 인공 뉴런 (컴퓨터 속 뉴런)

# 실제 뉴런처럼 작동하는 작은 계산기!

# 인공 뉴런이 하는 일:
# 1. 숫자들을 받음 (입력)
# 2. 중요한 건 크게, 안 중요한 건 작게 만듦 (가중치)
# 3. 다 더함
# 4. 결과를 다음 뉴런에게 전달

# 예시: 과일 구분하기
# 입력: 빨간색(0.9), 둥근 모양(0.8), 단맛(0.7)
# 뉴런이 판단: 0.9 + 0.8 + 0.7 = 2.4 → "사과 같은데?"


# 신경망 = 뉴런들을 층층이 쌓은 것

# 비유: 회사 조직도처럼!

#     [입력층]           [은닉층1]        [은닉층2]        [출력층]
#  사진 데이터    →    선 찾기    →    모양 찾기   →    사과/바나나
#   (픽셀들)         (패턴 인식)      (특징 조합)      (최종 판단)


# 각 층의 역할

# 1. 입력층 (Input Layer)
# = 데이터를 받아들이는 입구
# 예) 사진이라면 각 픽셀의 색깔 값
# 예) 숫자 4개를 입력 → 뉴런 4개

# 2. 은닉층 (Hidden Layer)  ← "딥"러닝의 핵심!
# = 중간에서 열심히 일하는 층
# 예) 1층: 선 찾기, 2층: 모양 찾기, 3층: 부분 찾기
# 층이 많을수록 복잡한 걸 배울 수 있음!

# 3. 출력층 (Output Layer)
# = 최종 답을 내는 층
# 예) 사과일 확률 80%, 바나나일 확률 20%

import torch.nn as nn

# 간단한 신경망 만들기
model = nn.Sequential(
    nn.Linear(4, 8),   # 입력 4개 → 은닉층 8개
    nn.ReLU(),         # 활성화 (나중에 배움)
    nn.Linear(8, 3),   # 은닉층 8개 → 출력 3개
)

print("=== 만든 신경망 구조 ===")
print(model)
print("\n각 층 설명:")
print("- Linear(4, 8): 입력 4개를 받아서 8개로 변환")
print("- ReLU(): 음수는 0으로 만들기 (활성화 함수)")
print("- Linear(8, 3): 8개를 받아서 최종 3개 출력")


# 가중치란? (Weight)

# 비유: 음악 믹서기의 볼륨 조절기

# 노래를 믹싱할 때:
# - 보컬 볼륨: 크게 (중요!)
# - 기타 볼륨: 중간
# - 북 볼륨: 작게

# 신경망도 똑같음:
# - 중요한 입력: 가중치를 크게 (예: 2.5)
# - 안 중요한 입력: 가중치를 작게 (예: 0.1)

# 예시: 사과 판별하기
# 빨간색(0.9) × 가중치(3.0) = 2.7  ← 색깔이 중요!
# 크기(0.5) × 가중치(0.2) = 0.1    ← 크기는 별로 안 중요


# 편향이란? (Bias)

# 비유: 기준점 조절

# 예) 시험 합격 기준
# - 60점 이상 합격
# - 하지만 "가산점 +5점" 주면?
# - 55점도 합격 가능!

# 편향 = 기준점을 올리거나 내림
# - 편향이 +5 → 더 쉽게 활성화
# - 편향이 -5 → 더 어렵게 활성화


# 파라미터 개수 세기 (쉽게!)

# 파라미터 = 학습을 통해 조절되는 값 (가중치 + 편향)

# 예시: 층1에서 층2로
# 층1: 뉴런 3개
# 층2: 뉴런 2개

# 필요한 가중치:
# - 층1의 뉴런1 → 층2의 뉴런1, 뉴런2 (2개)
# - 층1의 뉴런2 → 층2의 뉴런1, 뉴런2 (2개)
# - 층1의 뉴런3 → 층2의 뉴런1, 뉴런2 (2개)
# 총 가중치: 3 × 2 = 6개

# 필요한 편향:
# - 층2의 각 뉴런마다 1개 = 2개

# 총 파라미터 = 6 + 2 = 8개!

# 공식: (입력 개수 × 출력 개수) + 출력 개수

import torch
import torch.nn as nn

# 파라미터 개수 확인하기
model = nn.Sequential(
    nn.Linear(4, 3),   # 파라미터: 4×3 + 3 = 15개
    nn.ReLU(),
    nn.Linear(3, 2),   # 파라미터: 3×2 + 2 = 8개
)

print("\n=== 파라미터 개수 확인 ===")
total_params = sum(p.numel() for p in model.parameters())
print(f"총 파라미터 개수: {total_params}개")
print("(15 + 8 = 23개)")

print("\n=== 층별 파라미터 상세 ===")
for i, (name, param) in enumerate(model.named_parameters()):
    print(f"{name}: {param.shape} → {param.numel()}개")

# 왜 파라미터 개수가 중요할까?
print("\n파라미터가 많으면:")
print("✓ 복잡한 패턴을 배울 수 있음")
print("✗ 학습이 느림")
print("✗ 메모리를 많이 씀")
print("\n파라미터가 적으면:")
print("✓ 학습이 빠름")
print("✓ 메모리를 적게 씀")
print("✗ 복잡한 것을 못 배울 수 있음")


# 정리

# 1. 뉴런 = 작은 계산기 (입력받고, 처리하고, 전달)
# 2. 층 = 뉴런들의 집합 (입력층, 은닉층, 출력층)
# 3. 가중치 = 중요도 조절 (중요한 건 크게, 안 중요한 건 작게)
# 4. 편향 = 기준점 조절 (활성화 임계값 조정)
# 5. 파라미터 = 학습으로 조절되는 값 (가중치 + 편향)

print("\n" + "="*50)
print("핵심 정리")
print("="*50)
print("신경망 = 여러 층의 뉴런들이 협력하는 시스템")
print("학습 = 가중치와 편향을 조절해서 더 정확하게 만들기")
print("="*50)
